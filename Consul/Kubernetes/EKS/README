############### Deploy Consul on EKS using terraform #########
References:-
https://developer.hashicorp.com/consul/tutorials/get-started-kubernetes/kubernetes-gs-deploy

git clone https://github.com/hashicorp-education/learn-consul-get-started-kubernetes
cd learn-consul-get-started-kubernetes/self-managed/eks
terraform init
terraform applyhttps://github.com/abhineet-1987/Abhineet/tree/main/Consul/Kubernetes/EKS
aws eks --region $(terraform output -raw region) update-kubeconfig --name $(terraform output -raw kubernetes_cluster_id)
helm repo add hashicorp https://helm.releases.hashicorp.com
helm install --values helm/values-v1.yaml consul hashicorp/consul --create-namespace --namespace consul 

Note: If the Consul server pods are in PENDING stage due to PVC, storage class issue, 
kubectl apply -f gp3-storageclass.yaml
and then run helm upgrade or uninstall consul release using helm and install again

export CONSUL_HTTP_TOKEN=$(kubectl get --namespace consul secrets/consul-bootstrap-acl-token --template={{.data.token}} | base64 -d)
export CONSUL_HTTP_ADDR=https://$(kubectl get services/consul-ui --namespace consul -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
#######
Note: if CONSUL_HTTP_ADDR is blank/incomplete then,
kubectl port-forward service/consul-server --namespace consul 8501:8501
export CONSUL_HTTP_ADDR="https://localhost:8501"

Refer: https://developer.hashicorp.com/consul/docs/deploy/server/k8s/helm#viewing-the-consul-ui
##########
export CONSUL_HTTP_SSL_VERIFY=false
consul catalog services
consul members

-----------
If you need to setup 2 EKS clusters for connectivity for eg WAN federation MESH gateway setup, peering etc. You need to make sure that using above method
VPC will get CIDR range of 10.0.0.0/16
But if you want make VPC1 talk to VPC2, while deploying EKS2 update aws.tf to use non overlapping CIDR such as 10.1.0.0/16 if VPC1 CIDR is 10.0.0.0/16. Make sure you update subnets value as well in aws.tf accordingly.

Follow below:-
1. Create vpc peering connection
2. Accept the peering connection
3. Update VPC route tables' route for both the VPC by configuring destination the CIDR of other VPC and connection type as peering (pcx-) (update main (vpc) route table)
4. Update all the subnets (associated with above VPCs) route tables' route for both the VPC's subnets by configuring destination the CIDR of other VPC and connection type as peering (pcx-) (update subnet route table)
5. For ssh into worker nodes update security group's inbound rule to allow traffic.
6. Update security group of both the VPCs EKS worker nodes by adding inbound rule to allow all traffic from the other VPC's security group.
7. Validate by SSHing into the EKS1 worker node and ping the private ip of the EKS2 worker node. It should be pingable.

------------



