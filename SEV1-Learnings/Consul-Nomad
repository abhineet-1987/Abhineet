########## Consul ################
Check for MinQuorum value in "consul operator autopilot get-config"
ASG issue in AWS. Check whether nodes are being provisioned through ASG and if it is working fine or not. You may also get information on scaling policies set for ASG.
Check/Increase cpu,memory,disk space of the nodes if resource crunch.
Check for Security groups and whether server nodes can talk to each other. 
Sometimes if managed through terraform, ansible or some other tools can update the config and cause problems.
Check whether IAM role to describe ec2 instances exist and is applied on the nodes for cloud auto join to work.

If server nodes not joining in consensus even after using peers.json or some unusual behaviour seen then 
surely there may be some issue related with cloud provider, networking issue, cpu or memory issue etc.
So its good to know at first hand only how the server nodes are getting provisioned in the customer's infrastructure, i.e whether using ASG, terraform, Ansible etc

Stale service instances can appear during upgrades for eg (consul,envoy and nomad). Fix - restart consul 
If Consul cluster looses quorum after the PVC migration (storage migration). Fix - Scaled down Consul servers, deleted persistent volumes (PVs), and scaled back up to restore quorum.

upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: TLS_error:|268436501:SSL routines:OPENSSL_internal:SSLV3_ALERT_CERTIFICATE_EXPIRED:TLS_error_end:TLS_error_end
Fix: Deleted older replica set and the associated pod for the upstream deployment, which fixed the issue.

For protocol mismatch with the target service. Fix - apply protocol configuration via ProxyDefaults, which resolved the mismatch.

Consul Production | v1.17.1+env | Disk I/O usage 90%-100%. The core issue is most likely a high volume of synchronized writes to the Consul KV store, which is overwhelming the disk I/O capacity of the cluster
Fix: The current IOPs of 3,000 should be increased to 10,000 to improve disk throughput and handle the high write volume as suggested in our document: https://developer.hashicorp.com/consul/tutorials/production-vms/reference-architecture#hardware-sizing-for-consul-servers

Consul as storage backend for vault: vault pods getting in crashloopback. Issue was resolved when the changes were reverted back to addresses.http = 0.0.0.0 in consul server config and HTTP port to be bound to all interfaces (including the pod IP), allowing probes to succeed .
Post this change, The vault cluster came back up and running.

Consul DNS resolution error: Remove local domain search configuration for systemd-resolved to allow .consul domain routing.The configuration Domains=local ~consul introduces a global search domain (local) that conflicts with systemd-resolved's special handling of .local as an mDNS suffix.
This interferes with correct stub domain routing for .consul lookups by causing the resolver to either append .local to short names or bypass the routing logic for .consul.  
This results in failed service discovery for Consul-registered services. The fix is to use only Domains=~consul, which ensures .consul queries are properly routed to the local Consul DNS server (127.0.0.1:8600) without interference from search domains.
(https://hashicorp.zendesk.com/agent/tickets/185695)

For OOM errors collect heap.pprof. OOM issue arises in future, please provide us with the debug bundle (make sure it captures goroutine.pprof and heap.pprof).
architectural changes and optimizations may help (https://hashicorp.zendesk.com/agent/tickets/189649)

Consul timeout occurs at additional startup of Consul client (post consul version upgrade)
Fix: consul.hcl
gossip_lan {
probe_interval = "15s"
probe_timeout = "10s"
}



Scenario 1:
--------------
Consul as storage backend for vault: 
 
broken Consul cluster that lost quorum during a storage migration
Confirmed Consul cluster had lost quorum after the PVC migration.
Scaled down Consul servers, deleted persistent volumes (PVs), and scaled back up to restore quorum.
Verified Consul came back online, but Vault failed startup with: certificate signed by unknonwn authority when connecting to Consul over https://<HOST_IP>:8501.
Attempted to update Vault’s Consul CA secrets and restart pods; errors persisted.
Applied tls_skip_verify = true in Vault’s Consul storage stanza to bypass bootstrap certificate validation.
Vault successfully started.
Reinitialized Vault on the DR secondary and successfully re-enabled DR replication.

Fix:-
The failure was caused by a TLS certificate validation error between Vault and Consul after the Consul cluster was reset during the storage migration
Immediate Fix: Applied tls_skip_verify = true in Vault’s Consul storage stanza to bypass bootstrap certificate validation. (https://hashicorp.zendesk.com/agent/tickets/193938)
When Consul has global.enableAutoEncrypt=true it uses its internal Connect CA in order to issue certificates for clients. 
Hypothesis: there may be a bug or edge case where the Vault server or Consul agent held stale certificate information from the prior Consul cluster in memory during the reset/rebuild, causing a mismatch when the new CA was issued.


Scenario 2:
---------------
Actions taken to resolve/mitigate :-
Initially, the Consul cluster was recovered by setting bootstrap_expect = 1 on one of the server agents. 
The remaining server agents were then able to join the cluster one by one, after clearing the contents of their respective data directories. 
Once all agents had joined, Raft log entries were successfully replicated, and the cluster became stable. 
However, starting Vault triggered a flood of requests, which overwhelmed the cluster and caused it to fail again. 
It became clear that the EC2 instances were under-resourced and could not handle the load during Vault's startup process.
A similar recovery process was performed once more to bring the cluster back online.
To prevent further failures, a new Consul cluster was created, and a snapshot was restored to it.
Each EC2 instance was stopped and its instance type was upgraded from m5.2xlarge to m5.4xlarge.
The additional CPU and memory resources provided by the larger instance type allowed Consul to handle the surge in requests from Vault during startup.
 
Root Cause :-
Prior to the outage, the cluster was experiencing ongoing issues, including:
Missing or invalid agent tokens required for coordinate updates.
An expired Consul license.
The upgrade version tag not being set.
However, these issues do not appear to have directly caused the cluster failure.
The actual root cause was that the EC2 instances hosting the Consul server agents ran out of disk space.
This disk exhaustion eventually led to the loss of cluster leadership.
Logs from 09/03 indicate a networking issue between the existing leader node (10.1.2.216) and the rest of the cluster.
During the incident, a follower node (10.1.2.221) transitioned to the candidate state, triggering a new leader election while the previous leader remained unreachable.
The suspected network disruption prevented proper communication between the leader and follower nodes.
As a result, Raft log entries could not be appended and began accumulating in temporary files.
These temporary files continued to grow until they filled the disk, causing:
Inability to persist new state.
Loss of quorum and leadership. 
Overall cluster failure.
----------------------------------

######### Important write up for customer during SEV1 ###########
NOTE:-
We strive to provide a thorough root cause analysis for any incidents that occur. However, it is important to note that a definitive root cause cannot always be guaranteed. To enhance our ability to diagnose and resolve issues effectively, we require debug-level logs from the server(s) and from any impacted services. These logs are crucial in giving us the best possible chance to pinpoint the underlying causes and implement appropriate solutions.
We prefer to receive debug-level logs during the incident, though we understand that this is not always possible. In such cases, please provide any available system logs.

 
