########## Consul ################
Check for MinQuorum value in "consul operator autopilot get-config"
ASG issue in AWS. Check whether nodes are being provisioned through ASG and if it is working fine or not. You may also get information on scaling policies set for ASG.
Check/Increase cpu,memory,disk space of the nodes if resource crunch.
Check for Security groups and whether server nodes can talk to each other. 
Sometimes if managed through terraform, ansible or some other tools can update the config and cause problems.
Check whether IAM role to describe ec2 instances exist and is applied on the nodes for cloud auto join to work.

If server nodes not joining in consensus even after using peers.json or some unusual behaviour seen then 
surely there may be some issue related with cloud provider, networking issue, cpu or memory issue etc.
So its good to know at first hand only how the server nodes are getting provisioned in the customer's infrastructure, i.e whether using ASG, terraform, Ansible etc

Stale service instances can appear during upgrades for eg (consul,envoy and nomad). Fix - restart consul 
If Consul cluster looses quorum after the PVC migration (storage migration). Fix - Scaled down Consul servers, deleted persistent volumes (PVs), and scaled back up to restore quorum.

upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: TLS_error:|268436501:SSL routines:OPENSSL_internal:SSLV3_ALERT_CERTIFICATE_EXPIRED:TLS_error_end:TLS_error_end
Fix: Deleted older replica set and the associated pod for the upstream deployment, which fixed the issue.

For protocol mismatch with the target service. Fix - apply protocol configuration via ProxyDefaults, which resolved the mismatch.

Consul Production | v1.17.1+env | Disk I/O usage 90%-100%. The core issue is most likely a high volume of synchronized writes to the Consul KV store, which is overwhelming the disk I/O capacity of the cluster
Fix: The current IOPs of 3,000 should be increased to 10,000 to improve disk throughput and handle the high write volume as suggested in our document: https://developer.hashicorp.com/consul/tutorials/production-vms/reference-architecture#hardware-sizing-for-consul-servers

Consul as storage backend for vault: vault pods getting in crashloopback. Issue was resolved when the changes were reverted back to addresses.http = 0.0.0.0 in consul server config and HTTP port to be bound to all interfaces (including the pod IP), allowing probes to succeed .
Post this change, The vault cluster came back up and running.

Consul DNS resolution error: Remove local domain search configuration for systemd-resolved to allow .consul domain routing.The configuration Domains=local ~consul introduces a global search domain (local) that conflicts with systemd-resolved's special handling of .local as an mDNS suffix.
This interferes with correct stub domain routing for .consul lookups by causing the resolver to either append .local to short names or bypass the routing logic for .consul.  
This results in failed service discovery for Consul-registered services. The fix is to use only Domains=~consul, which ensures .consul queries are properly routed to the local Consul DNS server (127.0.0.1:8600) without interference from search domains.
(https://hashicorp.zendesk.com/agent/tickets/185695)

For OOM errors collect heap.pprof. OOM issue arises in future, please provide us with the debug bundle (make sure it captures goroutine.pprof and heap.pprof).
architectural changes and optimizations may help (https://hashicorp.zendesk.com/agent/tickets/189649)

Consul timeout occurs at additional startup of Consul client (post consul version upgrade)
Fix: consul.hcl
gossip_lan {
probe_interval = "15s"
probe_timeout = "10s"
}

check for the license expiry and make sure updated license is being used.

Internal desc = Error adding/updating listener(s) svcn-dhs-csms-csca-miscservice-stg2:127.0.0.1:1020: cannot bind '127.0.0.1:1020': Permission denied.
Fix: The proxy configuration could not be updated due to insufficient permissions, as the binding to a port below 1024 requires root privileges. This led to a SAN mismatch because the certificates were not updated. After updating the port range to a value above 1024 by the customer, the configuration changes were applied successfully, and the TLS error was resolved. 
(https://hashicorp.zendesk.com/agent/tickets/171195)

In one case server and clients TLS certificates were signed by different intermediary CAs (while the Root CA is the same). Consul clearly does not like it, so certificates signed by a single CA should be issued for entire Consul cluster (servers and clients).

tls: bad certificate errors: Inspect the certificates, if server certificate is expired then do helm upgrade to regenerate the new server certificate.
https://developer.hashicorp.com/consul/docs/secure/encryption/tls/rotate/k8s  (https://hashicorp.zendesk.com/agent/tickets/172809)

Optimize consul template queries for static configuration(using max_stale):
Fix: consul {
address = "127.0.0.1:8500"
}
# Allow stale reads up to 60 minutes to reduce leader load
max_stale   = "60m"
#disables blocking queries when set to 0
block_query_wait = "0s"

Request error: method=PUT url=/v1/session/renew/"session-id" from=127.0.0.1:55546 error="Session id' not found"
Fix: Blocked communication on port 8301 on client agents led to surf health failure which in turn resulted in session loss. Ensure communication on port 8301 is open and remains open when defining security group rule. 

Consul as Vault backend: The production Vault cluster's UI intermittently fails with HTTP 500 server error codes. Simultaneously, Vault agent containers experience repeated restarts, indicated by exit code 137, due to failing liveness probe checks related to backend storage health (Consul). This issue impacts system stability and the availability of the Vault service.
Fix: AWS VPC CNI network policies were blocking required communication on ports 8300 and 8301 (both inbound and outbound) between Vault server agents (Consul clients) and the Consul server cluster. This network misconfiguration disrupted critical Consul-related operations, including Serf and Raft protocols necessary for cluster health and communication.

Cannot access the service throught API Gateway but can be accessed via localhost internally. (https://hashicorp.zendesk.com/agent/tickets/168690)
Fix: The root cause of the issue to be duplicate iptables rules that were pointing to port 8443. This was identified by running the following command
iptables -L -t nat -n -v  | grep 8443
This could have happened due to the unclean shutdown of the API Gateway allocation by Nomad, which resulted in the iptables rules not cleaned up properly.

At some point encryption keys were added to each agent, but each agent wasn't restarted to enable it. So when the two clients were restarted yesterday for other reasons, encryption was enabled and they coudn't reach the other agents in the cluster.
Once we disabled encryption on the two clients by setting encrypt_verify_incoming and encrypt_verify_outgoing to false, they were able to connect back to the cluster.

Consul as Vault's backend: Leader and follower logs showed runtime: out of memory errors. OOM error.
Fix: backends for Vault, and is continuously having a hard time with handling the load(specifically memory). It's important to address by increasing the memory 2 to 4 times more than the working set size. So since your alloc_bytes is ~23GB, y

Incorrect http/https ports configuration may cause 503 Service Unavailable Error

The issue may be related to external services, such as the load balancer and other components interacting with the Consul. After fixing these components and restarting the Consul client agent, the issue may get resolved

Scenario 1:
-------------
On 5/13 at market open, service autoscaling began, launching new pods but these pods were still initializing and hadn’t passed health checks. During this startup period, the API gateway began routing traffic to those not-yet-available pods, resulting in request failures and errors for the customer.
Actions taken to resolve/mitigate
Since the issue would resolve after a couple of minutes, we weren't able to look at the logs from the time of the failure. Hence, no action was taken when the incident was reported. Therefore, we agreed to review the logs from the Consul server cluster and the Envoy logs from the failing pods as well as API gateway the next time the issue happens or if it is reproducible in your test environment. 
Based on the observed symptoms from the time of the incident, it's recommended to test the following annotations in your cluster:
annotations:
consul.hashicorp.com/connect-inject: "true"
consul.hashicorp.com/enable-sidecar-proxy-lifecycle: "true"
consul.hashicorp.com/sidecar-proxy-lifecycle-startup-grace-period-seconds: "30"
These annotations ensure the Consul sidecar proxy is injected and managed with a defined startup grace period, delaying traffic to the application until the proxy is fully ready. This change should first be validated in a non-production environment to confirm it resolves the issue without unintended side effects.
(https://hashicorp.zendesk.com/agent/tickets/183542)

Scenario 2:
--------------
Consul as storage backend for vault: 
 
broken Consul cluster that lost quorum during a storage migration
Confirmed Consul cluster had lost quorum after the PVC migration.
Scaled down Consul servers, deleted persistent volumes (PVs), and scaled back up to restore quorum.
Verified Consul came back online, but Vault failed startup with: certificate signed by unknonwn authority when connecting to Consul over https://<HOST_IP>:8501.
Attempted to update Vault’s Consul CA secrets and restart pods; errors persisted.
Applied tls_skip_verify = true in Vault’s Consul storage stanza to bypass bootstrap certificate validation.
Vault successfully started.
Reinitialized Vault on the DR secondary and successfully re-enabled DR replication.

Fix:-
The failure was caused by a TLS certificate validation error between Vault and Consul after the Consul cluster was reset during the storage migration
Immediate Fix: Applied tls_skip_verify = true in Vault’s Consul storage stanza to bypass bootstrap certificate validation. (https://hashicorp.zendesk.com/agent/tickets/193938)
When Consul has global.enableAutoEncrypt=true it uses its internal Connect CA in order to issue certificates for clients. 
Hypothesis: there may be a bug or edge case where the Vault server or Consul agent held stale certificate information from the prior Consul cluster in memory during the reset/rebuild, causing a mismatch when the new CA was issued.


Scenario 3:
---------------
Actions taken to resolve/mitigate :-
Initially, the Consul cluster was recovered by setting bootstrap_expect = 1 on one of the server agents. 
The remaining server agents were then able to join the cluster one by one, after clearing the contents of their respective data directories. 
Once all agents had joined, Raft log entries were successfully replicated, and the cluster became stable. 
However, starting Vault triggered a flood of requests, which overwhelmed the cluster and caused it to fail again. 
It became clear that the EC2 instances were under-resourced and could not handle the load during Vault's startup process.
A similar recovery process was performed once more to bring the cluster back online.
To prevent further failures, a new Consul cluster was created, and a snapshot was restored to it.
Each EC2 instance was stopped and its instance type was upgraded from m5.2xlarge to m5.4xlarge.
The additional CPU and memory resources provided by the larger instance type allowed Consul to handle the surge in requests from Vault during startup.
 
Root Cause :-
Prior to the outage, the cluster was experiencing ongoing issues, including:
Missing or invalid agent tokens required for coordinate updates.
An expired Consul license.
The upgrade version tag not being set.
However, these issues do not appear to have directly caused the cluster failure.
The actual root cause was that the EC2 instances hosting the Consul server agents ran out of disk space.
This disk exhaustion eventually led to the loss of cluster leadership.
Logs from 09/03 indicate a networking issue between the existing leader node (10.1.2.216) and the rest of the cluster.
During the incident, a follower node (10.1.2.221) transitioned to the candidate state, triggering a new leader election while the previous leader remained unreachable.
The suspected network disruption prevented proper communication between the leader and follower nodes.
As a result, Raft log entries could not be appended and began accumulating in temporary files.
These temporary files continued to grow until they filled the disk, causing:
Inability to persist new state.
Loss of quorum and leadership. 
Overall cluster failure.
----------------------------------

######### Important write up for customer during SEV1 ###########
NOTE:-
We strive to provide a thorough root cause analysis for any incidents that occur. However, it is important to note that a definitive root cause cannot always be guaranteed. To enhance our ability to diagnose and resolve issues effectively, we require debug-level logs from the server(s) and from any impacted services. These logs are crucial in giving us the best possible chance to pinpoint the underlying causes and implement appropriate solutions.
We prefer to receive debug-level logs during the incident, though we understand that this is not always possible. In such cases, please provide any available system logs.

 
