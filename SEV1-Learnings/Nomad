Error: 1) port not up 2) Cannot access the service through API Gateway
Fix: Stale IPtables rules, The issue seems to have happened after Node heartbeat miss alerts from nomad client to nomad server. We have identified a potential bug with Nomad not removing stale iptable rules when static ports have been assigned to job and  when nomad client loses connectivity with nomad server.
Checked the health of API gateways and upstreams, both appeared healthy.
Since this is a repeated incident, we discovered a stale API gateway IP address and port entry which was unreachable via curl on port 8443.
We identified the node that duplicate API gateway IP table entry
We cleaned up the IPtables rules containing stale IP address entries:
Listed IPtables rules for port 8443 : iptables -L -t nat -n -v  | grep 8443
Took note of target in offending rules , identified the bad IP address, cleaned its references by deleting the each such rule from all Chains: iptables -t nat -D <chain-name> <rule-num>
Flushed the offending IPtables CHAIN relevant to target captured: iptables -t nat -F <chain-name>
Deleted the Iptables CHAIN as well: iptables -t nat -X <chain-name>
Later verified that all the offending iptables rules containing stale IP address dont exist anymore using : iptables -L -t nat -n -v  | grep 8443
(Ticket: 187557, 176128)

Error: Closer look at the failing jobs logs revealed that Nomad is not able to pull a configured Envoy image. The private registry returns 404 "not found"
Fix: It was established that the private registry was reconfigured recently, so now it requires authentication. Unauthenticated requests get rejected.
No abnormalities in Nomad or Consul behaviour were noticed.
The customer team decided to proceed with manual distribution of the target image and reconfiguring jobs to use it locally instead of pulling it from the registry.

Error: When deploying a new service with Nomad, the existing Nomad containers do not terminate and remain in a pending state, resulting in 502 page errors. 
Fix: task “web”
config {
  init = true
}

Error: jobs stuck in pending state.
Fix: Verified that stopping and purging jobs did not resolve the issue.
Restarted the affected Nomad client, which resolved the immediate issue and allowed jobs to start running.
he application was exiting with status 0, leading to perpetual restarts by Nomad.  The exit code indicates the application was terminating as expected and nomad was just re-schedule per the jobspec. 

Error: nomad: error reloading server TLS configuration: error="listen tcp 0.0.0.0:4647: bind: address already in use"
Fix: Restart nomad agent on all server nodes.

Error: All jobs started failing after changing instance type from c6id.32xl to i7ie.48xl
Fix: Windows environment-specific issue rather than a Nomad defect. The errors (exit status 0xc0000142, plugin startup failures) originate from the Windows OS—specifically from failures in the Windows process startup sequence

Error: Not ready to serve consistent reads
Fix: Stopping the Nomad agent on the previous leader triggered a new leader to be elected and cluster quorum to form. (if this does not work then peers.json recovery)
For RCA ask customer for:
Nomad Debug Bundle from all servers with -stale argument
nomad debug -stale
The output of the following command from all server agents
nomad operator api /v1/agent/pprof/goroutine\?debug=1
